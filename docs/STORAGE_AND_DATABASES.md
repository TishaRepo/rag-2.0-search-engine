# ğŸ’¾ Storage & Vector Databases - Complete Guide

## ğŸ“ Where Are Your Documents Stored?

Here's your current storage structure:

```
data/
â”œâ”€â”€ raw/                              â† ğŸ“„ YOUR ORIGINAL DOCUMENTS
â”‚   â”œâ”€â”€ artificial_intelligence.json
â”‚   â”œâ”€â”€ machine_learning.json
â”‚   â”œâ”€â”€ natural_language_processing.json
â”‚   â”œâ”€â”€ retrieval_augmented_generation.json
â”‚   â”œâ”€â”€ vector_databases.json
â”‚   â””â”€â”€ asset-052018030468.txt        â† Your asset report!
â”‚
â”œâ”€â”€ processed/                        â† (Future: processed/cleaned data)
â”‚
â””â”€â”€ indices/                          â† ğŸ” SEARCH INDICES
    â”‚
    â”œâ”€â”€ bm25_index.pkl               â† BM25 keyword index (7KB)
    â”‚
    â””â”€â”€ chroma/                       â† VECTOR DATABASE
        â”œâ”€â”€ chroma.sqlite3           â† Metadata storage (397KB)
        â””â”€â”€ 3e89467f-.../            â† Vector data folder
            â”œâ”€â”€ data_level0.bin      â† Actual vectors (167KB)
            â”œâ”€â”€ header.bin           â† Index header info
            â”œâ”€â”€ length.bin           â† Vector lengths
            â””â”€â”€ link_lists.bin       â† HNSW graph links
```

---

## ğŸ“¦ Understanding Each File Type

### 1ï¸âƒ£ `.pkl` File (Pickle) - `bm25_index.pkl`

**What is it?**
- Python's way of saving objects to disk
- "Pickle" = serialize Python objects â†’ save to file

**What's inside?**
```python
# Our BM25 index pickle contains:
{
    "corpus": [
        ["artificial", "intelligence", "is", ...],  # Tokenized doc 1
        ["machine", "learning", "ml", ...],         # Tokenized doc 2
        ...
    ],
    "chunk_ids": ["abc123", "def456", ...]          # IDs to map back
}
```

**How it's generated?**
```python
import pickle

data = {"corpus": [...], "chunk_ids": [...]}
with open("bm25_index.pkl", "wb") as f:
    pickle.dump(data, f)  # Save to file
```

---

### 2ï¸âƒ£ `.sqlite3` File - `chroma.sqlite3`

**What is it?**
- SQLite database (lightweight, file-based SQL database)
- ChromaDB uses this to store **metadata** (not vectors!)

**What's inside?**
```sql
-- Tables in chroma.sqlite3:
collections      -- Your collection "ngse_documents"
embeddings       -- Mapping: chunk_id â†’ metadata
documents        -- Original text content
```

**Example data:**
| chunk_id | document | metadata |
|----------|----------|----------|
| abc123 | "AI is intelligence..." | {"title": "AI", "source": "..."} |
| def456 | "ML is a type of AI..." | {"title": "ML", "source": "..."} |

---

### 3ï¸âƒ£ `.bin` Files (Binary) - The Actual Vectors!

**What are they?**

| File | Purpose |
|------|---------|
| `data_level0.bin` | **The actual vectors!** All 384-dim vectors stored as raw bytes |
| `header.bin` | Index configuration (dimension, capacity, etc.) |
| `length.bin` | Number of vectors at each level |
| `link_lists.bin` | HNSW graph connections (for fast search) |

**How vectors are stored:**
```
Each vector = 384 floats Ã— 4 bytes = 1,536 bytes

data_level0.bin structure:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector 1: [0.12, -0.34, 0.56, ..., 0.23] (384 floats)   â”‚
â”‚ Vector 2: [0.45, 0.12, -0.78, ..., 0.67] (384 floats)   â”‚
â”‚ Vector 3: ...                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—„ï¸ What is ChromaDB?

### Overview

ChromaDB is an **open-source vector database** designed for AI applications.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ChromaDB                                 â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   SQLite    â”‚    â”‚   HNSW      â”‚    â”‚   Python API        â”‚  â”‚
â”‚  â”‚  (Metadata) â”‚    â”‚  (Vectors)  â”‚    â”‚   (Easy to use)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How ChromaDB Works Internally

#### Step 1: Adding Documents
```python
collection.add(
    ids=["chunk1"],
    embeddings=[[0.12, -0.34, ..., 0.23]],  # 384 numbers
    documents=["AI is intelligence..."],
    metadatas=[{"title": "AI"}]
)
```

What happens:
1. **SQLite**: Stores the document text and metadata
2. **HNSW Index**: Stores the vector in `.bin` files

#### Step 2: Searching
```python
results = collection.query(
    query_embeddings=[[0.15, -0.32, ..., 0.21]],
    n_results=5
)
```

What happens:
1. **HNSW Search**: Find 5 nearest vectors in `.bin` files
2. **SQLite Lookup**: Get documents/metadata for those vectors

### What is HNSW?

**HNSW = Hierarchical Navigable Small World**

It's the algorithm that makes vector search FAST!

```
Traditional Search:         HNSW Search:
Compare with ALL vectors    Navigate through layers
                           
O (â—) (â—) (â—) (â—) (â—)      Level 2:  (â—)â”€â”€â”€â”€â”€â”€â”€(â—)
  â†‘   â†‘   â†‘   â†‘   â†‘                    â†“
  Check each one            Level 1:  (â—)â”€â”€(â—)â”€â”€(â—)
  (slow for millions!)                  â†“
                           Level 0: (â—)(â—)(â—)(â—)(â—)
                                         â†‘
                           Found!        Jump to nearest
                           (Fast! O(log n))
```

---

## ğŸ¤” Why ChromaDB? (vs Alternatives)

### Comparison Table

| Feature | ChromaDB | FAISS | Weaviate | Pinecone |
|---------|----------|-------|----------|----------|
| **Setup** | â­ Easiest | Medium | Complex | Cloud only |
| **Local/Cloud** | Local | Local | Both | Cloud only |
| **Cost** | Free | Free | Free/Paid | Paid |
| **Persistence** | Built-in | Manual | Built-in | Cloud |
| **Metadata** | âœ… Yes | âŒ No | âœ… Yes | âœ… Yes |
| **Python API** | â­ Best | Good | Good | Good |
| **Best For** | Prototyping, Small-Medium | Maximum Speed | Production | Enterprise |

### Detailed Comparison

#### ğŸ”µ FAISS (Facebook AI Similarity Search)
```python
# FAISS is FAST but requires more work
import faiss

index = faiss.IndexFlatL2(384)  # Create index
index.add(vectors)              # Add vectors
distances, indices = index.search(query, k=5)  # Search

# Problems:
# âŒ No built-in metadata storage (you manage separately)
# âŒ No persistence by default (you save/load manually)
# âŒ Just indices, no documents returned
```

**When to use**: Need maximum speed, millions of vectors, willing to manage complexity

#### ğŸŸ¢ Weaviate
```python
# Weaviate is powerful but complex
import weaviate

client = weaviate.Client("http://localhost:8080")  # Needs server running!

client.schema.create_class({
    "class": "Document",
    "properties": [{"name": "content", "dataType": ["text"]}]
})

# Problems:
# âŒ Requires running a separate server (Docker)
# âŒ More complex schema definition
# âŒ Overkill for small projects
```

**When to use**: Production systems, need GraphQL API, complex schemas

#### ğŸŸ¡ Pinecone
```python
# Pinecone is cloud-only
import pinecone

pinecone.init(api_key="your-key", environment="us-east-1")
index = pinecone.Index("my-index")

index.upsert(vectors=[("id1", [0.1, 0.2, ...], {"metadata": "..."})])

# Problems:
# âŒ Cloud only (data leaves your machine)
# âŒ Costs money for production use
# âŒ Requires internet connection
```

**When to use**: Enterprise, don't want to manage infrastructure, have budget

#### ğŸŸ£ ChromaDB (Our Choice!)
```python
# ChromaDB is simple and complete
import chromadb

client = chromadb.PersistentClient(path="./data")  # One line!
collection = client.create_collection("docs")

collection.add(
    ids=["id1"],
    embeddings=[[0.1, 0.2, ...]],
    documents=["Original text"],      # âœ… Stores text!
    metadatas=[{"source": "file.txt"}]  # âœ… Stores metadata!
)

results = collection.query(
    query_embeddings=[[0.1, 0.2, ...]],
    n_results=5,
    include=["documents", "metadatas"]  # âœ… Returns everything!
)
```

**Why we chose ChromaDB:**
- âœ… **Zero setup** - Just `pip install chromadb`
- âœ… **Persistence built-in** - Saves automatically
- âœ… **Stores everything** - Vectors, documents, metadata
- âœ… **Simple API** - Easy to learn and use
- âœ… **Local** - Your data stays on your machine
- âœ… **Free** - Open source, no costs
- âœ… **Good for learning** - Perfect for understanding RAG

---

## ğŸ”§ How Files Are Generated

### The Complete Flow

```python
# In indexer.py:

# 1. Initialize ChromaDB with persistence
client = chromadb.PersistentClient(path="data/indices/chroma")
#                                       â†“
#                        Creates: chroma.sqlite3
#                                 {uuid}/data_level0.bin, etc.

# 2. Create/get collection
collection = client.get_or_create_collection("ngse_documents")
#                                                    â†“
#                        Adds entry to: chroma.sqlite3 (collections table)

# 3. Add documents
collection.add(
    ids=["chunk1", "chunk2"],
    embeddings=[[...], [...]],  # 384-dim vectors
    documents=["text1", "text2"],
    metadatas=[{...}, {...}]
)
#     â†“
# chroma.sqlite3: Stores documents + metadata
# data_level0.bin: Stores vectors (raw bytes)
# link_lists.bin: Builds HNSW graph for fast search

# 4. Save BM25 index (separate from ChromaDB)
import pickle
with open("data/indices/bm25_index.pkl", "wb") as f:
    pickle.dump({"corpus": tokenized_docs, "chunk_ids": ids}, f)
```

---

## ğŸ“Š Size Comparison

For your 6 documents:

| File | Size | What's in it |
|------|------|--------------|
| `chroma.sqlite3` | 397 KB | Metadata, document text, mappings |
| `data_level0.bin` | 167 KB | 6 vectors Ã— 384 dims Ã— 4 bytes + overhead |
| `bm25_index.pkl` | 7 KB | Tokenized words + IDs |

**Scaling estimate:**
- 1,000 documents â†’ ~30 MB
- 100,000 documents â†’ ~3 GB
- 1,000,000 documents â†’ ~30 GB (consider FAISS/Pinecone)

---

## ğŸ¯ Summary

| Question | Answer |
|----------|--------|
| Where are documents? | `data/raw/` (originals), `data/indices/` (searchable) |
| What is `.pkl`? | Python pickle - serializes BM25 index |
| What is `.sqlite3`? | SQLite database - stores metadata & text |
| What are `.bin` files? | Raw binary vectors for HNSW search |
| Why ChromaDB? | Simple, local, free, stores everything, perfect for learning |
| When to switch? | FAISS for speed, Pinecone for cloud, Weaviate for production |

---

## ğŸ”„ Want to Try FAISS Instead?

I can show you how to add FAISS as an alternative! It's already in your `requirements.txt`:
```
faiss-cpu
```

Just let me know if you want to see the comparison in code!
